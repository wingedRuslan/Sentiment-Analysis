{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaggle  - Word2Vec Tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the data\n",
    "train = pd.read_csv(\"labeledTrainData.tsv\", header=0, delimiter=\"\\t\", quoting=3)\n",
    "test = pd.read_csv(\"testData.tsv\", header=0, delimiter=\"\\t\", quoting=3)\n",
    "\n",
    "# since Word2Vec can learn from unlabeled data, these extra 50,000 reviews can now be used\n",
    "#  unlabeledTrain.tsv, which contains 50,000 additional reviews with no labels\n",
    "unlabeled_train = pd.read_csv(\"unlabeledTrainData.tsv\", header=0, delimiter=\"\\t\", quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 2)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unlabeled_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unlabeled_train Data ---  ['id', 'review']\n",
      "Train Data ---  ['id', 'sentiment', 'review']\n",
      "Test Data ---  ['id', 'review']\n"
     ]
    }
   ],
   "source": [
    "print(\"Unlabeled_train Data --- \", list(unlabeled_train.columns.values))\n",
    "print(\"Train Data --- \", list(train.columns.values))\n",
    "print(\"Test Data --- \", list(test.columns.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total 100.000 reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make a specific input format  //// Prepare our data for input to Word2Vec\n",
    "** Word2Vec - input format is a list of lists **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to split a paragraph into sentences ---> use NLTK's punkt tokenizer for sentence splitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk.data\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sentence_splitting import review_to_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = []  # Initialize an empty list of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences from training set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pilot/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:219: UserWarning: \"b'.'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "/home/pilot/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.happierabroad.com\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    }
   ],
   "source": [
    "print(\"Parsing sentences from training set\")\n",
    "for review in train[\"review\"]:\n",
    "    sentences += review_to_sentences(review, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences from unlabeled set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pilot/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:219: UserWarning: \"b'.'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "/home/pilot/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.archive.org/details/LovefromaStranger\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/home/pilot/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.loosechangeguide.com/LooseChangeGuide.html\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/home/pilot/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.msnbc.msn.com/id/4972055/site/newsweek/\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/home/pilot/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:219: UserWarning: \"b'..'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "/home/pilot/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.youtube.com/watch?v=a0KSqelmgN8\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/home/pilot/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://jake-weird.blogspot.com/2007/08/beneath.html\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    }
   ],
   "source": [
    "print(\"Parsing sentences from unlabeled set\")\n",
    "for review in unlabeled_train[\"review\"]:\n",
    "    sentences += review_to_sentences(review, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "795553\n"
     ]
    }
   ],
   "source": [
    "print(len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['with', 'all', 'this', 'stuff', 'going', 'down', 'at', 'the', 'moment', 'with', 'mj', 'i', 've', 'started', 'listening', 'to', 'his', 'music', 'watching', 'the', 'odd', 'documentary', 'here', 'and', 'there', 'watched', 'the', 'wiz', 'and', 'watched', 'moonwalker', 'again']\n"
     ]
    }
   ],
   "source": [
    "print(sentences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Saving The Model\n",
    "- With the list of nicely parsed sentences, we're ready to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import the built-in logging module and configure it so that Word2Vec \n",
    "# creates nice output messages\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set values for various parameters\n",
    "num_features = 300    # Word vector dimensionality\n",
    "min_word_count = 40   # Minimum word count\n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 10          # Context window size     \n",
    "downsampling = 1e-3   # Downsample setting for frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-10 19:56:24,958 : INFO : collecting all words and their counts\n",
      "2019-05-10 19:56:24,959 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is training and money is rolling af\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-10 19:56:25,043 : INFO : PROGRESS: at sentence #10000, processed 226019 words, keeping 17775 word types\n",
      "2019-05-10 19:56:25,135 : INFO : PROGRESS: at sentence #20000, processed 452117 words, keeping 24945 word types\n",
      "2019-05-10 19:56:25,209 : INFO : PROGRESS: at sentence #30000, processed 671351 words, keeping 30027 word types\n",
      "2019-05-10 19:56:25,285 : INFO : PROGRESS: at sentence #40000, processed 897864 words, keeping 34346 word types\n",
      "2019-05-10 19:56:25,352 : INFO : PROGRESS: at sentence #50000, processed 1117056 words, keeping 37759 word types\n",
      "2019-05-10 19:56:25,422 : INFO : PROGRESS: at sentence #60000, processed 1338402 words, keeping 40716 word types\n",
      "2019-05-10 19:56:25,491 : INFO : PROGRESS: at sentence #70000, processed 1561619 words, keeping 43321 word types\n",
      "2019-05-10 19:56:25,562 : INFO : PROGRESS: at sentence #80000, processed 1780986 words, keeping 45713 word types\n",
      "2019-05-10 19:56:25,631 : INFO : PROGRESS: at sentence #90000, processed 2004982 words, keeping 48129 word types\n",
      "2019-05-10 19:56:25,700 : INFO : PROGRESS: at sentence #100000, processed 2227023 words, keeping 50203 word types\n",
      "2019-05-10 19:56:25,773 : INFO : PROGRESS: at sentence #110000, processed 2446618 words, keeping 52079 word types\n",
      "2019-05-10 19:56:25,853 : INFO : PROGRESS: at sentence #120000, processed 2668959 words, keeping 54116 word types\n",
      "2019-05-10 19:56:25,930 : INFO : PROGRESS: at sentence #130000, processed 2894389 words, keeping 55845 word types\n",
      "2019-05-10 19:56:25,990 : INFO : PROGRESS: at sentence #140000, processed 3106963 words, keeping 57345 word types\n",
      "2019-05-10 19:56:26,073 : INFO : PROGRESS: at sentence #150000, processed 3332574 words, keeping 59050 word types\n",
      "2019-05-10 19:56:26,147 : INFO : PROGRESS: at sentence #160000, processed 3555468 words, keeping 60614 word types\n",
      "2019-05-10 19:56:26,228 : INFO : PROGRESS: at sentence #170000, processed 3778890 words, keeping 62077 word types\n",
      "2019-05-10 19:56:26,300 : INFO : PROGRESS: at sentence #180000, processed 3999417 words, keeping 63496 word types\n",
      "2019-05-10 19:56:26,408 : INFO : PROGRESS: at sentence #190000, processed 4224599 words, keeping 64794 word types\n",
      "2019-05-10 19:56:26,522 : INFO : PROGRESS: at sentence #200000, processed 4448857 words, keeping 66086 word types\n",
      "2019-05-10 19:56:26,604 : INFO : PROGRESS: at sentence #210000, processed 4669951 words, keeping 67388 word types\n",
      "2019-05-10 19:56:26,677 : INFO : PROGRESS: at sentence #220000, processed 4895065 words, keeping 68694 word types\n",
      "2019-05-10 19:56:26,771 : INFO : PROGRESS: at sentence #230000, processed 5117727 words, keeping 69957 word types\n",
      "2019-05-10 19:56:26,885 : INFO : PROGRESS: at sentence #240000, processed 5345119 words, keeping 71164 word types\n",
      "2019-05-10 19:56:26,955 : INFO : PROGRESS: at sentence #250000, processed 5559290 words, keeping 72351 word types\n",
      "2019-05-10 19:56:27,025 : INFO : PROGRESS: at sentence #260000, processed 5779354 words, keeping 73478 word types\n",
      "2019-05-10 19:56:27,102 : INFO : PROGRESS: at sentence #270000, processed 6000640 words, keeping 74765 word types\n",
      "2019-05-10 19:56:27,176 : INFO : PROGRESS: at sentence #280000, processed 6226447 words, keeping 76368 word types\n",
      "2019-05-10 19:56:27,251 : INFO : PROGRESS: at sentence #290000, processed 6449474 words, keeping 77833 word types\n",
      "2019-05-10 19:56:27,330 : INFO : PROGRESS: at sentence #300000, processed 6674144 words, keeping 79171 word types\n",
      "2019-05-10 19:56:27,406 : INFO : PROGRESS: at sentence #310000, processed 6899390 words, keeping 80478 word types\n",
      "2019-05-10 19:56:27,480 : INFO : PROGRESS: at sentence #320000, processed 7124318 words, keeping 81807 word types\n",
      "2019-05-10 19:56:27,555 : INFO : PROGRESS: at sentence #330000, processed 7345966 words, keeping 83022 word types\n",
      "2019-05-10 19:56:27,627 : INFO : PROGRESS: at sentence #340000, processed 7575555 words, keeping 84279 word types\n",
      "2019-05-10 19:56:27,702 : INFO : PROGRESS: at sentence #350000, processed 7798904 words, keeping 85423 word types\n",
      "2019-05-10 19:56:27,776 : INFO : PROGRESS: at sentence #360000, processed 8019200 words, keeping 86592 word types\n",
      "2019-05-10 19:56:27,857 : INFO : PROGRESS: at sentence #370000, processed 8246875 words, keeping 87708 word types\n",
      "2019-05-10 19:56:27,929 : INFO : PROGRESS: at sentence #380000, processed 8471796 words, keeping 88876 word types\n",
      "2019-05-10 19:56:28,003 : INFO : PROGRESS: at sentence #390000, processed 8701541 words, keeping 89906 word types\n",
      "2019-05-10 19:56:28,076 : INFO : PROGRESS: at sentence #400000, processed 8924561 words, keeping 90914 word types\n",
      "2019-05-10 19:56:28,154 : INFO : PROGRESS: at sentence #410000, processed 9145994 words, keeping 91880 word types\n",
      "2019-05-10 19:56:28,230 : INFO : PROGRESS: at sentence #420000, processed 9367120 words, keeping 92912 word types\n",
      "2019-05-10 19:56:28,309 : INFO : PROGRESS: at sentence #430000, processed 9594395 words, keeping 93929 word types\n",
      "2019-05-10 19:56:28,387 : INFO : PROGRESS: at sentence #440000, processed 9821157 words, keeping 94904 word types\n",
      "2019-05-10 19:56:28,464 : INFO : PROGRESS: at sentence #450000, processed 10045087 words, keeping 96034 word types\n",
      "2019-05-10 19:56:28,535 : INFO : PROGRESS: at sentence #460000, processed 10277908 words, keeping 97087 word types\n",
      "2019-05-10 19:56:28,607 : INFO : PROGRESS: at sentence #470000, processed 10505677 words, keeping 97933 word types\n",
      "2019-05-10 19:56:28,678 : INFO : PROGRESS: at sentence #480000, processed 10725930 words, keeping 98859 word types\n",
      "2019-05-10 19:56:28,754 : INFO : PROGRESS: at sentence #490000, processed 10952814 words, keeping 99871 word types\n",
      "2019-05-10 19:56:28,825 : INFO : PROGRESS: at sentence #500000, processed 11174510 words, keeping 100765 word types\n",
      "2019-05-10 19:56:28,895 : INFO : PROGRESS: at sentence #510000, processed 11399762 words, keeping 101697 word types\n",
      "2019-05-10 19:56:28,973 : INFO : PROGRESS: at sentence #520000, processed 11623139 words, keeping 102597 word types\n",
      "2019-05-10 19:56:29,041 : INFO : PROGRESS: at sentence #530000, processed 11847423 words, keeping 103399 word types\n",
      "2019-05-10 19:56:29,115 : INFO : PROGRESS: at sentence #540000, processed 12072078 words, keeping 104262 word types\n",
      "2019-05-10 19:56:29,188 : INFO : PROGRESS: at sentence #550000, processed 12297781 words, keeping 105133 word types\n",
      "2019-05-10 19:56:29,262 : INFO : PROGRESS: at sentence #560000, processed 12518996 words, keeping 105997 word types\n",
      "2019-05-10 19:56:29,339 : INFO : PROGRESS: at sentence #570000, processed 12747788 words, keeping 106784 word types\n",
      "2019-05-10 19:56:29,418 : INFO : PROGRESS: at sentence #580000, processed 12969400 words, keeping 107663 word types\n",
      "2019-05-10 19:56:29,505 : INFO : PROGRESS: at sentence #590000, processed 13194978 words, keeping 108498 word types\n",
      "2019-05-10 19:56:29,603 : INFO : PROGRESS: at sentence #600000, processed 13417305 words, keeping 109218 word types\n",
      "2019-05-10 19:56:29,709 : INFO : PROGRESS: at sentence #610000, processed 13638342 words, keeping 110091 word types\n",
      "2019-05-10 19:56:29,791 : INFO : PROGRESS: at sentence #620000, processed 13864627 words, keeping 110835 word types\n",
      "2019-05-10 19:56:29,865 : INFO : PROGRESS: at sentence #630000, processed 14088807 words, keeping 111601 word types\n",
      "2019-05-10 19:56:29,960 : INFO : PROGRESS: at sentence #640000, processed 14309603 words, keeping 112416 word types\n",
      "2019-05-10 19:56:30,076 : INFO : PROGRESS: at sentence #650000, processed 14535418 words, keeping 113196 word types\n",
      "2019-05-10 19:56:30,166 : INFO : PROGRESS: at sentence #660000, processed 14758404 words, keeping 113945 word types\n",
      "2019-05-10 19:56:30,246 : INFO : PROGRESS: at sentence #670000, processed 14981622 words, keeping 114640 word types\n",
      "2019-05-10 19:56:30,327 : INFO : PROGRESS: at sentence #680000, processed 15206533 words, keeping 115353 word types\n",
      "2019-05-10 19:56:30,398 : INFO : PROGRESS: at sentence #690000, processed 15428725 words, keeping 116126 word types\n",
      "2019-05-10 19:56:30,469 : INFO : PROGRESS: at sentence #700000, processed 15657437 words, keeping 116943 word types\n",
      "2019-05-10 19:56:30,539 : INFO : PROGRESS: at sentence #710000, processed 15880206 words, keeping 117596 word types\n",
      "2019-05-10 19:56:30,610 : INFO : PROGRESS: at sentence #720000, processed 16105636 words, keeping 118221 word types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-10 19:56:30,684 : INFO : PROGRESS: at sentence #730000, processed 16331861 words, keeping 118954 word types\n",
      "2019-05-10 19:56:30,756 : INFO : PROGRESS: at sentence #740000, processed 16552718 words, keeping 119665 word types\n",
      "2019-05-10 19:56:30,819 : INFO : PROGRESS: at sentence #750000, processed 16771272 words, keeping 120295 word types\n",
      "2019-05-10 19:56:30,897 : INFO : PROGRESS: at sentence #760000, processed 16990689 words, keeping 120929 word types\n",
      "2019-05-10 19:56:30,973 : INFO : PROGRESS: at sentence #770000, processed 17217953 words, keeping 121702 word types\n",
      "2019-05-10 19:56:31,049 : INFO : PROGRESS: at sentence #780000, processed 17448128 words, keeping 122402 word types\n",
      "2019-05-10 19:56:31,128 : INFO : PROGRESS: at sentence #790000, processed 17675202 words, keeping 123066 word types\n",
      "2019-05-10 19:56:31,173 : INFO : collected 123504 word types from a corpus of 17798519 raw words and 795553 sentences\n",
      "2019-05-10 19:56:31,174 : INFO : Loading a fresh vocabulary\n",
      "2019-05-10 19:56:31,283 : INFO : effective_min_count=40 retains 16490 unique words (13% of original 123504, drops 107014)\n",
      "2019-05-10 19:56:31,283 : INFO : effective_min_count=40 leaves 17239365 word corpus (96% of original 17798519, drops 559154)\n",
      "2019-05-10 19:56:31,404 : INFO : deleting the raw counts dictionary of 123504 items\n",
      "2019-05-10 19:56:31,410 : INFO : sample=0.001 downsamples 48 most-common words\n",
      "2019-05-10 19:56:31,411 : INFO : downsampling leaves estimated 12749967 word corpus (74.0% of prior 17239365)\n",
      "2019-05-10 19:56:31,496 : INFO : estimated required memory for 16490 words and 300 dimensions: 47821000 bytes\n",
      "2019-05-10 19:56:31,497 : INFO : resetting layer weights\n",
      "2019-05-10 19:56:31,802 : INFO : training model with 4 workers on 16490 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2019-05-10 19:56:32,814 : INFO : EPOCH 1 - PROGRESS: at 4.21% examples, 534126 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 19:56:33,819 : INFO : EPOCH 1 - PROGRESS: at 7.36% examples, 466104 words/s, in_qsize 8, out_qsize 0\n",
      "2019-05-10 19:56:34,822 : INFO : EPOCH 1 - PROGRESS: at 10.92% examples, 460599 words/s, in_qsize 8, out_qsize 0\n",
      "2019-05-10 19:56:35,823 : INFO : EPOCH 1 - PROGRESS: at 15.01% examples, 473969 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 19:56:36,842 : INFO : EPOCH 1 - PROGRESS: at 18.02% examples, 453447 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 19:56:37,846 : INFO : EPOCH 1 - PROGRESS: at 22.32% examples, 467954 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 19:56:38,857 : INFO : EPOCH 1 - PROGRESS: at 26.62% examples, 478893 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 19:56:39,893 : INFO : EPOCH 1 - PROGRESS: at 30.68% examples, 481128 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 19:56:40,901 : INFO : EPOCH 1 - PROGRESS: at 34.94% examples, 486718 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 19:56:41,913 : INFO : EPOCH 1 - PROGRESS: at 38.91% examples, 488233 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 19:56:42,939 : INFO : EPOCH 1 - PROGRESS: at 42.97% examples, 490147 words/s, in_qsize 8, out_qsize 1\n",
      "2019-05-10 19:56:43,961 : INFO : EPOCH 1 - PROGRESS: at 47.33% examples, 494795 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 19:56:44,962 : INFO : EPOCH 1 - PROGRESS: at 50.00% examples, 483249 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 19:56:45,979 : INFO : EPOCH 1 - PROGRESS: at 53.67% examples, 481361 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 19:56:46,994 : INFO : EPOCH 1 - PROGRESS: at 57.84% examples, 484928 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 19:56:48,006 : INFO : EPOCH 1 - PROGRESS: at 62.25% examples, 489516 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 19:56:49,014 : INFO : EPOCH 1 - PROGRESS: at 66.63% examples, 493264 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 19:56:50,021 : INFO : EPOCH 1 - PROGRESS: at 69.94% examples, 489158 words/s, in_qsize 8, out_qsize 0\n",
      "2019-05-10 19:56:51,035 : INFO : EPOCH 1 - PROGRESS: at 73.23% examples, 485352 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 19:56:52,055 : INFO : EPOCH 1 - PROGRESS: at 77.72% examples, 489145 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 19:56:53,062 : INFO : EPOCH 1 - PROGRESS: at 82.15% examples, 492530 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 19:56:54,068 : INFO : EPOCH 1 - PROGRESS: at 86.59% examples, 495696 words/s, in_qsize 6, out_qsize 1\n",
      "2019-05-10 19:56:55,073 : INFO : EPOCH 1 - PROGRESS: at 90.98% examples, 498549 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 19:56:56,077 : INFO : EPOCH 1 - PROGRESS: at 95.17% examples, 499719 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 19:56:57,079 : INFO : EPOCH 1 - PROGRESS: at 99.54% examples, 502283 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 19:56:57,144 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-05-10 19:56:57,154 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-10 19:56:57,172 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-10 19:56:57,176 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-10 19:56:57,178 : INFO : EPOCH - 1 : training on 17798519 raw words (12748684 effective words) took 25.4s, 502553 effective words/s\n",
      "2019-05-10 19:56:58,201 : INFO : EPOCH 2 - PROGRESS: at 4.32% examples, 545803 words/s, in_qsize 6, out_qsize 1\n",
      "2019-05-10 19:56:59,206 : INFO : EPOCH 2 - PROGRESS: at 8.83% examples, 557376 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 19:57:00,218 : INFO : EPOCH 2 - PROGRESS: at 13.25% examples, 555404 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 19:57:01,224 : INFO : EPOCH 2 - PROGRESS: at 17.69% examples, 555367 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 19:57:02,233 : INFO : EPOCH 2 - PROGRESS: at 22.15% examples, 556357 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 19:57:03,243 : INFO : EPOCH 2 - PROGRESS: at 26.62% examples, 558032 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 19:57:04,247 : INFO : EPOCH 2 - PROGRESS: at 31.03% examples, 557549 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 19:57:05,263 : INFO : EPOCH 2 - PROGRESS: at 35.50% examples, 557293 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 19:57:06,264 : INFO : EPOCH 2 - PROGRESS: at 39.91% examples, 558063 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 19:57:07,275 : INFO : EPOCH 2 - PROGRESS: at 44.26% examples, 557415 words/s, in_qsize 6, out_qsize 1\n",
      "2019-05-10 19:57:08,281 : INFO : EPOCH 2 - PROGRESS: at 48.66% examples, 557807 words/s, in_qsize 6, out_qsize 1\n",
      "2019-05-10 19:57:09,290 : INFO : EPOCH 2 - PROGRESS: at 53.11% examples, 557918 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 19:57:10,296 : INFO : EPOCH 2 - PROGRESS: at 57.47% examples, 558177 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 19:57:11,316 : INFO : EPOCH 2 - PROGRESS: at 61.87% examples, 557897 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 19:57:12,332 : INFO : EPOCH 2 - PROGRESS: at 66.34% examples, 558236 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 19:57:13,345 : INFO : EPOCH 2 - PROGRESS: at 70.77% examples, 558207 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 19:57:14,354 : INFO : EPOCH 2 - PROGRESS: at 75.19% examples, 558281 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 19:57:15,361 : INFO : EPOCH 2 - PROGRESS: at 79.52% examples, 557645 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 19:57:16,374 : INFO : EPOCH 2 - PROGRESS: at 83.56% examples, 555036 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 19:57:17,377 : INFO : EPOCH 2 - PROGRESS: at 87.91% examples, 555121 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 19:57:18,379 : INFO : EPOCH 2 - PROGRESS: at 92.28% examples, 555189 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 19:57:19,397 : INFO : EPOCH 2 - PROGRESS: at 96.80% examples, 555475 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 19:57:20,089 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-05-10 19:57:20,106 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-10 19:57:20,117 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-10 19:57:20,123 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-10 19:57:20,125 : INFO : EPOCH - 2 : training on 17798519 raw words (12748709 effective words) took 22.9s, 555934 effective words/s\n",
      "2019-05-10 19:57:21,135 : INFO : EPOCH 3 - PROGRESS: at 4.21% examples, 534653 words/s, in_qsize 7, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-10 19:57:22,145 : INFO : EPOCH 3 - PROGRESS: at 8.67% examples, 546885 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 19:57:23,158 : INFO : EPOCH 3 - PROGRESS: at 13.19% examples, 553040 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 19:57:24,171 : INFO : EPOCH 3 - PROGRESS: at 17.75% examples, 556067 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 19:57:25,190 : INFO : EPOCH 3 - PROGRESS: at 22.20% examples, 555761 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 19:57:26,192 : INFO : EPOCH 3 - PROGRESS: at 26.62% examples, 557002 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 19:57:27,198 : INFO : EPOCH 3 - PROGRESS: at 31.14% examples, 558567 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 19:57:28,198 : INFO : EPOCH 3 - PROGRESS: at 35.56% examples, 558331 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 19:57:29,211 : INFO : EPOCH 3 - PROGRESS: at 39.96% examples, 558361 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 19:57:30,223 : INFO : EPOCH 3 - PROGRESS: at 44.38% examples, 558365 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 19:57:31,235 : INFO : EPOCH 3 - PROGRESS: at 48.77% examples, 558331 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 19:57:32,245 : INFO : EPOCH 3 - PROGRESS: at 53.27% examples, 558960 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 19:57:33,250 : INFO : EPOCH 3 - PROGRESS: at 57.63% examples, 559215 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 19:57:34,264 : INFO : EPOCH 3 - PROGRESS: at 62.03% examples, 559116 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 19:57:35,267 : INFO : EPOCH 3 - PROGRESS: at 66.45% examples, 559359 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 19:57:36,274 : INFO : EPOCH 3 - PROGRESS: at 70.77% examples, 558579 words/s, in_qsize 6, out_qsize 1\n",
      "2019-05-10 19:57:37,278 : INFO : EPOCH 3 - PROGRESS: at 75.02% examples, 557574 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 19:57:38,280 : INFO : EPOCH 3 - PROGRESS: at 79.40% examples, 557523 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 19:57:39,281 : INFO : EPOCH 3 - PROGRESS: at 83.84% examples, 557892 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 19:57:40,292 : INFO : EPOCH 3 - PROGRESS: at 88.23% examples, 557905 words/s, in_qsize 8, out_qsize 0\n",
      "2019-05-10 19:57:41,301 : INFO : EPOCH 3 - PROGRESS: at 92.68% examples, 558031 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 19:57:42,312 : INFO : EPOCH 3 - PROGRESS: at 97.18% examples, 558386 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 19:57:42,916 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-05-10 19:57:42,930 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-10 19:57:42,940 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-10 19:57:42,951 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-10 19:57:42,952 : INFO : EPOCH - 3 : training on 17798519 raw words (12749437 effective words) took 22.8s, 558700 effective words/s\n",
      "2019-05-10 19:57:43,963 : INFO : EPOCH 4 - PROGRESS: at 4.26% examples, 543459 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 19:57:44,972 : INFO : EPOCH 4 - PROGRESS: at 8.72% examples, 551371 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 19:57:45,979 : INFO : EPOCH 4 - PROGRESS: at 13.19% examples, 554757 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 19:57:46,986 : INFO : EPOCH 4 - PROGRESS: at 17.75% examples, 558211 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 19:57:48,003 : INFO : EPOCH 4 - PROGRESS: at 22.20% examples, 557787 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 19:57:49,013 : INFO : EPOCH 4 - PROGRESS: at 26.62% examples, 557908 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 19:57:50,020 : INFO : EPOCH 4 - PROGRESS: at 31.09% examples, 558281 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 19:57:51,026 : INFO : EPOCH 4 - PROGRESS: at 35.56% examples, 558625 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 19:57:52,032 : INFO : EPOCH 4 - PROGRESS: at 39.96% examples, 559070 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 19:57:53,041 : INFO : EPOCH 4 - PROGRESS: at 44.38% examples, 559150 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 19:57:54,062 : INFO : EPOCH 4 - PROGRESS: at 48.82% examples, 559235 words/s, in_qsize 8, out_qsize 0\n",
      "2019-05-10 19:57:55,065 : INFO : EPOCH 4 - PROGRESS: at 53.27% examples, 559561 words/s, in_qsize 8, out_qsize 0\n",
      "2019-05-10 19:57:56,077 : INFO : EPOCH 4 - PROGRESS: at 57.63% examples, 559389 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 19:57:57,079 : INFO : EPOCH 4 - PROGRESS: at 61.70% examples, 556634 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 19:57:58,092 : INFO : EPOCH 4 - PROGRESS: at 66.12% examples, 556744 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 19:57:59,096 : INFO : EPOCH 4 - PROGRESS: at 70.55% examples, 557174 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 19:58:00,112 : INFO : EPOCH 4 - PROGRESS: at 74.96% examples, 557089 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 19:58:01,117 : INFO : EPOCH 4 - PROGRESS: at 79.34% examples, 557009 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 19:58:02,121 : INFO : EPOCH 4 - PROGRESS: at 83.78% examples, 557307 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 19:58:03,124 : INFO : EPOCH 4 - PROGRESS: at 88.18% examples, 557588 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 19:58:04,135 : INFO : EPOCH 4 - PROGRESS: at 92.62% examples, 557685 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 19:58:05,138 : INFO : EPOCH 4 - PROGRESS: at 97.07% examples, 557897 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 19:58:05,783 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-05-10 19:58:05,794 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-10 19:58:05,808 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-10 19:58:05,816 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-10 19:58:05,817 : INFO : EPOCH - 4 : training on 17798519 raw words (12751059 effective words) took 22.9s, 557919 effective words/s\n",
      "2019-05-10 19:58:06,842 : INFO : EPOCH 5 - PROGRESS: at 4.21% examples, 529240 words/s, in_qsize 8, out_qsize 2\n",
      "2019-05-10 19:58:07,843 : INFO : EPOCH 5 - PROGRESS: at 8.78% examples, 553599 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 19:58:08,853 : INFO : EPOCH 5 - PROGRESS: at 13.14% examples, 550887 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 19:58:09,867 : INFO : EPOCH 5 - PROGRESS: at 16.01% examples, 502992 words/s, in_qsize 6, out_qsize 1\n",
      "2019-05-10 19:58:10,871 : INFO : EPOCH 5 - PROGRESS: at 20.33% examples, 510444 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 19:58:11,876 : INFO : EPOCH 5 - PROGRESS: at 24.78% examples, 519138 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 19:58:12,878 : INFO : EPOCH 5 - PROGRESS: at 29.15% examples, 524359 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 19:58:13,886 : INFO : EPOCH 5 - PROGRESS: at 33.66% examples, 528803 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 19:58:14,889 : INFO : EPOCH 5 - PROGRESS: at 38.01% examples, 531923 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 19:58:15,890 : INFO : EPOCH 5 - PROGRESS: at 42.41% examples, 535077 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 19:58:16,901 : INFO : EPOCH 5 - PROGRESS: at 46.83% examples, 537273 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 19:58:17,912 : INFO : EPOCH 5 - PROGRESS: at 51.19% examples, 538473 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 19:58:18,913 : INFO : EPOCH 5 - PROGRESS: at 55.61% examples, 540479 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 19:58:19,917 : INFO : EPOCH 5 - PROGRESS: at 59.84% examples, 541017 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 19:58:20,922 : INFO : EPOCH 5 - PROGRESS: at 64.17% examples, 541426 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 19:58:21,930 : INFO : EPOCH 5 - PROGRESS: at 68.66% examples, 543055 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 19:58:22,931 : INFO : EPOCH 5 - PROGRESS: at 73.00% examples, 543892 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 19:58:23,944 : INFO : EPOCH 5 - PROGRESS: at 77.49% examples, 545048 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 19:58:24,946 : INFO : EPOCH 5 - PROGRESS: at 81.92% examples, 546010 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 19:58:25,954 : INFO : EPOCH 5 - PROGRESS: at 86.30% examples, 546410 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 19:58:26,968 : INFO : EPOCH 5 - PROGRESS: at 90.71% examples, 546913 words/s, in_qsize 7, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-10 19:58:27,969 : INFO : EPOCH 5 - PROGRESS: at 95.17% examples, 547713 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 19:58:28,980 : INFO : EPOCH 5 - PROGRESS: at 99.61% examples, 548516 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 19:58:29,036 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-05-10 19:58:29,038 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-10 19:58:29,046 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-10 19:58:29,052 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-10 19:58:29,054 : INFO : EPOCH - 5 : training on 17798519 raw words (12749083 effective words) took 23.2s, 548911 effective words/s\n",
      "2019-05-10 19:58:29,061 : INFO : training on a 88992595 raw words (63746972 effective words) took 117.3s, 543649 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is trained...Fuck yeah\n"
     ]
    }
   ],
   "source": [
    "# Initialize and train the model\n",
    "from gensim.models import word2vec\n",
    "print(\"Model is training and money is rolling af\")\n",
    "model = word2vec.Word2Vec(sentences, workers=num_workers,size=num_features, min_count=min_word_count,window=context, sample=downsampling)\n",
    "print(\"Model is trained...Fuck yeah\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-10 22:09:02,562 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    }
   ],
   "source": [
    "# If you don't plan to train the model any further, calling \n",
    "# init_sims will make the model much more memory-efficient.\n",
    "model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-10 22:10:25,450 : INFO : saving Word2Vec object under 300features_40minwords_10context, separately None\n",
      "2019-05-10 22:10:25,515 : INFO : not storing attribute vectors_norm\n",
      "2019-05-10 22:10:25,553 : INFO : not storing attribute cum_table\n",
      "2019-05-10 22:10:25,554 : WARNING : this function is deprecated, use smart_open.open instead\n",
      "2019-05-10 22:10:26,095 : INFO : saved 300features_40minwords_10context\n"
     ]
    }
   ],
   "source": [
    "# сreate a model name and \n",
    "# save the model for later use. to load it later use Word2Vec.load()\n",
    "model_name = \"300features_40minwords_10context\"\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check out the created model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pilot/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `doesnt_match` (Method will be removed in 4.0.0, use self.wv.doesnt_match() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/home/pilot/anaconda3/lib/python3.6/site-packages/gensim/models/keyedvectors.py:876: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  vectors = vstack(self.word_vec(word, use_norm=True) for word in used_words).astype(REAL)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'kitchen'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.doesnt_match(\"man woman child kitchen\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pilot/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `doesnt_match` (Method will be removed in 4.0.0, use self.wv.doesnt_match() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/home/pilot/anaconda3/lib/python3.6/site-packages/gensim/models/keyedvectors.py:876: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  vectors = vstack(self.word_vec(word, use_norm=True) for word in used_words).astype(REAL)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'berlin'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.doesnt_match(\"france england germany berlin\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pilot/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `doesnt_match` (Method will be removed in 4.0.0, use self.wv.doesnt_match() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/home/pilot/anaconda3/lib/python3.6/site-packages/gensim/models/keyedvectors.py:876: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  vectors = vstack(self.word_vec(word, use_norm=True) for word in used_words).astype(REAL)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'paris'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.doesnt_match(\"paris berlin london austria\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('woman', 0.6361821889877319),\n",
       " ('lady', 0.5945104360580444),\n",
       " ('lad', 0.5891189575195312),\n",
       " ('monk', 0.5783478617668152),\n",
       " ('chap', 0.527835488319397),\n",
       " ('millionaire', 0.5215615034103394),\n",
       " ('guy', 0.5157663226127625),\n",
       " ('farmer', 0.5132026672363281),\n",
       " ('soldier', 0.5089080929756165),\n",
       " ('men', 0.5038425922393799)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(\"man\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('princess', 0.6379745602607727),\n",
       " ('maid', 0.5924339890480042),\n",
       " ('victoria', 0.5898520946502686),\n",
       " ('bride', 0.5822241902351379),\n",
       " ('showgirl', 0.5775257349014282),\n",
       " ('mistress', 0.5619137287139893),\n",
       " ('eva', 0.5530508756637573),\n",
       " ('countess', 0.5480133295059204),\n",
       " ('fatale', 0.54304039478302),\n",
       " ('starlet', 0.5383342504501343)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(\"queen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('terrible', 0.7853543162345886),\n",
       " ('horrible', 0.7502487897872925),\n",
       " ('atrocious', 0.7373061180114746),\n",
       " ('horrendous', 0.7108439207077026),\n",
       " ('abysmal', 0.7098503112792969),\n",
       " ('dreadful', 0.7081030607223511),\n",
       " ('appalling', 0.6880141496658325),\n",
       " ('horrid', 0.6760132312774658),\n",
       " ('lousy', 0.6248980760574341),\n",
       " ('laughable', 0.6150200366973877)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(\"awful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model is capable of distinguishing differences in meaning! **The model is trained to understand semantic understanding of words**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Docs - Word2Vec model has a feature vector for each word in the vocabulary, stored in a numpy array called \"syn0\" - updated - \"model.wv.vectors\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pilot/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.vectors instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model.wv.syn0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pilot/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.vectors instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(16490, 300)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.syn0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model.wv.vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16490, 300)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of rows is the number of words in the model's vocabulary, and the number of columns corresponds to the size of the feature vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pilot/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.00740212,  0.10584512,  0.00791013,  0.01596658,  0.02414292,\n",
       "        0.02893355,  0.1469221 ,  0.03602852,  0.00566807, -0.04403942,\n",
       "       -0.03476333,  0.07901409, -0.0525446 ,  0.05537491, -0.01916643,\n",
       "        0.08392595,  0.05564789,  0.00716173,  0.05965544, -0.01869686,\n",
       "        0.0145442 ,  0.06206767,  0.01221299, -0.07976333, -0.01082874,\n",
       "       -0.03657145,  0.01086357,  0.01661276, -0.02401651, -0.00501237,\n",
       "        0.00330291,  0.01660081, -0.11201243, -0.03290961,  0.00296581,\n",
       "       -0.07590261,  0.01477253,  0.03373786, -0.12277917,  0.04188901,\n",
       "       -0.01258932,  0.0175688 ,  0.03103006,  0.00069398, -0.03225007,\n",
       "       -0.03792301,  0.05510093,  0.07353526,  0.01316158,  0.04571639,\n",
       "        0.07401988,  0.00230472, -0.06641226, -0.00203784, -0.09438307,\n",
       "        0.00041785, -0.03966195, -0.04849317,  0.04449907, -0.05610603,\n",
       "        0.14267004,  0.04641591,  0.01891815, -0.04306285, -0.05566777,\n",
       "        0.0195535 ,  0.05449059, -0.07047797,  0.06007987, -0.02455452,\n",
       "        0.06597472, -0.00172762, -0.0452909 , -0.02796619,  0.02245177,\n",
       "       -0.02882012,  0.01914154,  0.06736813, -0.05626521,  0.08465685,\n",
       "        0.10363181,  0.0325193 , -0.01534639,  0.03205078,  0.04991215,\n",
       "        0.07764567,  0.02028338,  0.0328865 ,  0.08660008, -0.05477956,\n",
       "        0.1059496 ,  0.00907145, -0.03977168,  0.02901562, -0.00373198,\n",
       "       -0.08643845, -0.01672378, -0.08808883,  0.00057135, -0.05471685,\n",
       "       -0.08499865, -0.00185272, -0.00229681, -0.07652692,  0.10095587,\n",
       "       -0.00475801, -0.03357412, -0.01984303,  0.00036236, -0.05442275,\n",
       "        0.01862674, -0.09609454,  0.01861902, -0.07040129, -0.07340381,\n",
       "        0.0679045 , -0.07535872,  0.02897212,  0.04768676, -0.02254521,\n",
       "       -0.03368574, -0.08450512,  0.02852992,  0.0329642 ,  0.01452547,\n",
       "       -0.04187977, -0.008735  , -0.05302897,  0.1289397 ,  0.04802037,\n",
       "        0.024634  ,  0.00934676,  0.00773235,  0.05820603, -0.02063313,\n",
       "       -0.01624824, -0.02883671,  0.02381045,  0.00950695,  0.03214963,\n",
       "        0.07288361, -0.11705132, -0.05452229,  0.05325363, -0.01149454,\n",
       "        0.01565318, -0.09896618,  0.06484576, -0.0323936 , -0.05514896,\n",
       "       -0.04407446, -0.09938237,  0.0580567 ,  0.00599956, -0.0334855 ,\n",
       "       -0.05995296, -0.02230237,  0.01896472,  0.00098379,  0.0181558 ,\n",
       "        0.12574562, -0.03273975,  0.08180056,  0.07039753,  0.0021057 ,\n",
       "        0.02915604, -0.00851747,  0.02611442,  0.02706963, -0.00593761,\n",
       "       -0.11915649, -0.00279283, -0.02882758,  0.06039194,  0.05506223,\n",
       "        0.02065473,  0.01084546,  0.04793282, -0.05337164, -0.01384155,\n",
       "        0.01993611,  0.01633034,  0.09696674, -0.01589004, -0.04413375,\n",
       "       -0.00852241,  0.04132888, -0.04289086,  0.06458645,  0.00114112,\n",
       "       -0.02402196,  0.00842251, -0.00291004, -0.08260893,  0.0078329 ,\n",
       "       -0.00725519,  0.04206541,  0.00535015,  0.1072934 , -0.08824369,\n",
       "       -0.10044228, -0.04641369,  0.00615499,  0.06930833, -0.11389375,\n",
       "        0.07810239, -0.02012494,  0.00722093,  0.01156854, -0.01699272,\n",
       "       -0.10961601,  0.07268711,  0.02848434,  0.06180465, -0.01454481,\n",
       "        0.09559823, -0.0408952 ,  0.01959777,  0.09747944,  0.07246149,\n",
       "       -0.02406908,  0.03903992,  0.05443219, -0.03349376,  0.16704631,\n",
       "        0.01522748, -0.04767587,  0.02108975,  0.04112561,  0.05534492,\n",
       "        0.06006571, -0.00480002,  0.055479  , -0.00145585, -0.00392347,\n",
       "        0.0317524 , -0.02019568,  0.01808212, -0.00642657, -0.0115962 ,\n",
       "        0.1119835 , -0.05988441,  0.03631897,  0.11605398,  0.03899719,\n",
       "       -0.06002567, -0.0477194 , -0.13528645, -0.13077103,  0.10741595,\n",
       "       -0.01748403, -0.05640331, -0.0154489 , -0.00932243, -0.04307843,\n",
       "        0.01298015, -0.06379607, -0.06722168,  0.00492324, -0.00603294,\n",
       "       -0.03565912, -0.04334139,  0.04910726, -0.06226603, -0.042689  ,\n",
       "       -0.17420675, -0.07000447, -0.04243692, -0.09276025,  0.10707807,\n",
       "        0.11181742, -0.02420363,  0.06664787, -0.00927038, -0.00365056,\n",
       "        0.02003815, -0.14465722, -0.00765194,  0.09376914,  0.03616738,\n",
       "       -0.00332754,  0.03940729,  0.01335546,  0.02396488,  0.01951822,\n",
       "       -0.05209522,  0.015928  ,  0.02423667,  0.18589881,  0.05282824,\n",
       "        0.03168435,  0.02527758, -0.09628906, -0.00863782, -0.09883079,\n",
       "       -0.06095301,  0.06758015, -0.05057535, -0.08285041,  0.03548059],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model[\"flower\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "each word is a vector in 300-dimensional space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From Words To Paragraphs, Attempt 1: Vector Averaging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate average feature vectors for training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cleanData_word2vec import review_to_wordlist\n",
    "clean_train_reviews = []\n",
    "for review in train[\"review\"]:\n",
    "    clean_train_reviews.append(review_to_wordlist(review, remove_stopwords=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from vecAveraging import getAvgFeatureVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 0.0 of 25000\n",
      "Review 1000.0 of 25000\n",
      "Review 2000.0 of 25000\n",
      "Review 3000.0 of 25000\n",
      "Review 4000.0 of 25000\n",
      "Review 5000.0 of 25000\n",
      "Review 6000.0 of 25000\n",
      "Review 7000.0 of 25000\n",
      "Review 8000.0 of 25000\n",
      "Review 9000.0 of 25000\n",
      "Review 10000.0 of 25000\n",
      "Review 11000.0 of 25000\n",
      "Review 12000.0 of 25000\n",
      "Review 13000.0 of 25000\n",
      "Review 14000.0 of 25000\n",
      "Review 15000.0 of 25000\n",
      "Review 16000.0 of 25000\n",
      "Review 17000.0 of 25000\n",
      "Review 18000.0 of 25000\n",
      "Review 19000.0 of 25000\n",
      "Review 20000.0 of 25000\n",
      "Review 21000.0 of 25000\n",
      "Review 22000.0 of 25000\n",
      "Review 23000.0 of 25000\n",
      "Review 24000.0 of 25000\n"
     ]
    }
   ],
   "source": [
    "trainDataVecs = getAvgFeatureVecs( clean_train_reviews, model, num_features )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating average feature vecs for test reviews\n",
      "Review 0.0 of 25000\n",
      "Review 1000.0 of 25000\n",
      "Review 2000.0 of 25000\n",
      "Review 3000.0 of 25000\n",
      "Review 4000.0 of 25000\n",
      "Review 5000.0 of 25000\n",
      "Review 6000.0 of 25000\n",
      "Review 7000.0 of 25000\n",
      "Review 8000.0 of 25000\n",
      "Review 9000.0 of 25000\n",
      "Review 10000.0 of 25000\n",
      "Review 11000.0 of 25000\n",
      "Review 12000.0 of 25000\n",
      "Review 13000.0 of 25000\n",
      "Review 14000.0 of 25000\n",
      "Review 15000.0 of 25000\n",
      "Review 16000.0 of 25000\n",
      "Review 17000.0 of 25000\n",
      "Review 18000.0 of 25000\n",
      "Review 19000.0 of 25000\n",
      "Review 20000.0 of 25000\n",
      "Review 21000.0 of 25000\n",
      "Review 22000.0 of 25000\n",
      "Review 23000.0 of 25000\n",
      "Review 24000.0 of 25000\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating average feature vecs for test reviews\")\n",
    "clean_test_reviews = []\n",
    "for review in test[\"review\"]:\n",
    "    clean_test_reviews.append(review_to_wordlist(review,remove_stopwords=True))\n",
    "\n",
    "testDataVecs = getAvgFeatureVecs(clean_test_reviews, model, num_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use the average paragraph vectors to train a random forest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pilot/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "# Fit a random forest to the training data, using 100 trees\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "forest = RandomForestClassifier( n_estimators = 100 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting a random forest to labeled training data...\n"
     ]
    }
   ],
   "source": [
    "print(\"Fitting a random forest to labeled training data...\")\n",
    "forest = forest.fit( trainDataVecs, train[\"sentiment\"] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test & extract results \n",
    "result = forest.predict(testDataVecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write the test results \n",
    "output = pd.DataFrame(data={\"id\":test[\"id\"], \"sentiment\":result})\n",
    "output.to_csv(\"Word2Vec_AverageVectors.csv\", index=False, quoting=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this produced results underperformed Bag of Words by a few percentage points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
